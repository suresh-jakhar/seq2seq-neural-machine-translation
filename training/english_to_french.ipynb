{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01ccce83-b58d-478e-9a57-1b8fbfe5ebd1",
   "metadata": {},
   "source": [
    "## 1) Install and imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35086631-c44e-4d24-91be-495e7526b417",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q sentencepiece tqdm sacrebleu\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentencepiece as spm\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "#This brief install block ensures the tokenizer and helper libraries are available.\n",
    "#sentencepiece is required; sacrebleu is optional for BLEU evaluation but recommended if internet access during environment creation is allowed. \n",
    "#If you plan to run on an offline system, install these dependencies beforehand.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8329002a-b88f-4958-8bd1-d2e05c36a769",
   "metadata": {},
   "source": [
    "##### This brief install block ensures the tokenizer and helper libraries are available. sentencepiece is required; sacrebleu is optional for BLEU evaluation but recommended if internet access during environment creation is allowed. If you plan to run on an offline system, install these dependencies beforehand."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b6c564-ca2c-46b0-bdea-f1f3fa40bf07",
   "metadata": {},
   "source": [
    "## 2) Environment and file paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43cf0548-4184-4a7a-803e-02b9b8f59693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Device selection\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# File paths (these are the outputs the notebook will produce)\n",
    "DATA_CSV = \"eng-french.csv\"         # input dataset (must be present)\n",
    "SPM_PREFIX = \"bpe_enfr\"             # SentencePiece model prefix (will produce .model and .vocab)\n",
    "SPM_MODEL = f\"{SPM_PREFIX}.model\"\n",
    "SPM_VOCAB  = f\"{SPM_PREFIX}.vocab\"\n",
    "CKPT_PATH = \"best_local_transformer.pt\"\n",
    "PICKLE_PATH = \"best_local_transformer.pkl\"\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if device == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ce0eea-9e45-4538-8a18-600782195e0b",
   "metadata": {},
   "source": [
    "##### The notebook is reproducible by setting a fixed random seed. GPU availability is auto-detected. Keep DATA_CSV in the same directory as the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a17d2e5-fcae-4828-813a-eb46aa3f0b48",
   "metadata": {},
   "source": [
    "## 3) Hyperparameters and design choices (tuned for a laptop GPU with 4-6 GB VRAM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9bdb343d-e233-40c5-a73b-4748e9c041d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters set.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Tokenizer\n",
    "VOCAB_SIZE = 8000   # chosen as a balance between expressiveness and memory\n",
    "\n",
    "# - Why 8000: good trade-off for general translation tasks on modest datasets; captures common subwords\n",
    "# - Alternative (advantage/cost): 16000 (better rare-word handling, more parameters and memory), 4000 (smaller memory but more fragmentation of common words)\n",
    "\n",
    "# Sequence lengths\n",
    "MAX_SRC_LEN = 50\n",
    "MAX_TGT_LEN = 50\n",
    "\n",
    "# - 50 tokens covers most short/medium sentences; larger lengths increase memory and computation quadratically in attention\n",
    "\n",
    "# Transformer architecture\n",
    "D_MODEL = 384      # model dimension\n",
    "NHEAD = 6          # number of attention heads (must divide D_MODEL)\n",
    "ENC_LAY = 4        # encoder layers\n",
    "DEC_LAY = 4        # decoder layers\n",
    "FF_DIM = 1024      # feed forward hidden dimension\n",
    "DROPOUT = 0.15     # regularization\n",
    "\n",
    "# - If you have more VRAM (e.g., T4 in Colab), you can scale D_MODEL and FF_DIM up for better accuracy.\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 64        # per step batch size; actual effective = BATCH_SIZE * GRAD_ACCUM_STEPS\n",
    "GRAD_ACCUM_STEPS = 3   # effective batch = 192 (64 * 3)\n",
    "EPOCHS = 45\n",
    "PATIENCE = 5\n",
    "LABEL_SMOOTHING = 0.05\n",
    "WARMUP_STEPS = 2500\n",
    "LR_BASE = 1.0\n",
    "WEIGHT_DECAY = 1e-4\n",
    "MAX_GRAD_NORM = 1.0\n",
    "\n",
    "\n",
    "# - Gradient accumulation improves effective batch size without OOM. Typical accum steps: 2-4 on small GPUs.\n",
    "# - Label smoothing prevents overconfidence; 0.05 is a conservative value that improves generalization.\n",
    "print(\"Hyperparameters set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97608d62-15ba-482e-888b-8baa9db9f851",
   "metadata": {},
   "source": [
    "##### Each hyperparameter includes a concise rationale and alternatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fdaaaa-c78f-4e9a-946a-bf4665a3eb9a",
   "metadata": {},
   "source": [
    "## 4) Load dataset and create plain text corpus for SentencePiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1daec2fd-585a-47d2-9a2e-3e7662ac7c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total examples: 175621\n",
      "Corpus for SentencePiece written to spm_corpus.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "assert Path(DATA_CSV).exists(), f\"Dataset file {DATA_CSV} not found.\"\n",
    "\n",
    "df = pd.read_csv(DATA_CSV)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Convert columns to strings and lowercase for consistency\n",
    "eng = df[\"English words/sentences\"].astype(str).str.strip().str.lower()\n",
    "fra = df[\"French words/sentences\"].astype(str).str.strip().str.lower()\n",
    "\n",
    "print(f\"Total examples: {len(df)}\")\n",
    "\n",
    "# Create a combined text file for SentencePiece training: include both languages to get a shared BPE\n",
    "CORPUS_PATH = \"spm_corpus.txt\"\n",
    "with open(CORPUS_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    for s in eng:\n",
    "        f.write(s + \"\\n\")\n",
    "    for s in fra:\n",
    "        f.write(s + \"\\n\")\n",
    "\n",
    "print(f\"Corpus for SentencePiece written to {CORPUS_PATH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19395c61-ef9c-441e-b01d-edcaa47e27be",
   "metadata": {},
   "source": [
    "##### A shared BPE (single tokenizer for both languages) can improve subword alignment and simplify model embeddings (single joint vocabulary). Alternative: separate tokenizers per language advantage: tailored tokenization per language; cost: larger embedding matrices and extra complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b13608b7-91c6-40ad-ac4e-1ae7ab3ecf51",
   "metadata": {},
   "source": [
    "## 5) Train SentencePiece (BPE) tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfddc9bc-cc44-4bc4-bc35-f7b8fa3bdc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece trained and loaded. Vocab size: 8000\n"
     ]
    }
   ],
   "source": [
    "# This produces bpe_enfr.model and bpe_enfr.vocab\n",
    "spm.SentencePieceTrainer.Train(\n",
    "    input=CORPUS_PATH,\n",
    "    model_prefix=SPM_PREFIX,\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    model_type='bpe',\n",
    "    character_coverage=1.0,\n",
    "    bos_id=1, eos_id=2, pad_id=0, unk_id=3\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load(SPM_MODEL)\n",
    "print(\"SentencePiece trained and loaded. Vocab size:\", sp.get_piece_size())\n",
    "\n",
    "# Save a short README about tokenizer choices\n",
    "with open(\"TOKENIZER_README.md\", \"w\") as f:\n",
    "    f.write(\"Tokenizer: SentencePiece (BPE)\\n\")\n",
    "    f.write(f\"vocab_size = {VOCAB_SIZE}\\n\")\n",
    "    f.write(\"Remarks: joint BPE for source and target languages.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2958fab7-907c-4f10-9dd8-be7cbaadefb0",
   "metadata": {},
   "source": [
    "##### Training the tokenizer inside the notebook ensures reproducibility. vocab_size=8000 is the chosen compromise. character_coverage=1.0 is fine for Latin scripts; for multilingual or special scripts, adjust accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ba0789-9532-4317-95af-b5e469a162d7",
   "metadata": {},
   "source": [
    "## 6) Encoding helpers and prepare numeric arrays for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec3f01ad-90a2-4aae-b384-bc0cc340926e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shapes: (140496, 50) (17562, 50) (17563, 50)\n"
     ]
    }
   ],
   "source": [
    "PAD_ID = 0\n",
    "BOS_ID = 1\n",
    "EOS_ID = 2\n",
    "\n",
    "def encode_sentence(text, maxlen):\n",
    "    # encode using sentencepiece; add BOS/EOS and pad/truncate\n",
    "    ids = [BOS_ID] + sp.encode(text, out_type=int)[:maxlen-2] + [EOS_ID]\n",
    "    if len(ids) < maxlen:\n",
    "        ids += [PAD_ID] * (maxlen - len(ids))\n",
    "    return ids[:maxlen]\n",
    "\n",
    "# Encode entire dataset into numpy arrays\n",
    "X = np.array([encode_sentence(s, MAX_SRC_LEN) for s in eng], dtype=np.int64)\n",
    "Y = np.array([encode_sentence(s, MAX_TGT_LEN) for s in fra], dtype=np.int64)\n",
    "\n",
    "# Train / Validation / Test split (80/10/10)\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_tmp, Y_train, Y_tmp = train_test_split(X, Y, test_size=0.20, random_state=SEED)\n",
    "X_valid, X_test, Y_valid, Y_test = train_test_split(X_tmp, Y_tmp, test_size=0.50, random_state=SEED)\n",
    "\n",
    "print(\"Dataset shapes:\", X_train.shape, X_valid.shape, X_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d156a8fc-14b4-4175-8073-859bfc648210",
   "metadata": {},
   "source": [
    "##### Split stratified by default randomness; for small datasets, consider k-fold cross validation as an alternative to obtain more robust estimates. Here we use a single holdout for simplicity and reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856a7353-3395-4f4a-9175-08cf17f3c471",
   "metadata": {},
   "source": [
    "## 7) Dataset and DataLoaders (teacher forcing: tgt_in and tgt_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "787d1c2d-9646-4c06-b4a7-1f6954eae69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NMTDataset(Dataset):\n",
    "    def __init__(self, src_array, tgt_array):\n",
    "        self.src = torch.tensor(src_array, dtype=torch.long)\n",
    "        self.tgt = torch.tensor(tgt_array, dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return self.src.size(0)\n",
    "    def __getitem__(self, idx):\n",
    "        src = self.src[idx]\n",
    "        tgt = self.tgt[idx]\n",
    "        # return source, decoder input (t=0..T-1), decoder target (t=1..T)\n",
    "        return src, tgt[:-1], tgt[1:]\n",
    "\n",
    "train_loader = DataLoader(NMTDataset(X_train, Y_train), batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "valid_loader = DataLoader(NMTDataset(X_valid, Y_valid), batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader  = DataLoader(NMTDataset(X_test,  Y_test),  batch_size=BATCH_SIZE, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7919720-1b81-48ee-8cc0-439cf4082fe1",
   "metadata": {},
   "source": [
    "##### drop_last=True during training ensures consistent batch sizes for gradient accumulation. For evaluation, keep drop_last=False to measure on full data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c4fbe8-96a2-407a-857c-b1e86bc22ddd",
   "metadata": {},
   "source": [
    "## 8) Model definition: positional encoding and seq2seq transformer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa6b9583-a526-43b9-abb2-ef45d0c95a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"Standard sinusoidal positional encoding as used in the Transformer paper.\n",
    "    max_len is set to MAX_LEN to match training sequence lengths; this avoids positional-buffer mismatch when loading checkpoints.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=MAX_SRC_LEN):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # shape [1, max_len, d_model]\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe[:, :x.size(1)])\n",
    "\n",
    "class TransformerNMT(nn.Module):\n",
    "    \"\"\"Encoder-decoder Transformer wrapper using torch.nn.Transformer (batch_first=True).\n",
    "    Uses a shared vocabulary (joint BPE) with tied embeddings for decoder and output projection.\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model, nhead, enc_layers, dec_layers, dim_feedforward, dropout):\n",
    "        super().__init__()\n",
    "        self.src_embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n",
    "        self.tgt_embed = nn.Embedding(vocab_size, d_model, padding_idx=PAD_ID)\n",
    "\n",
    "        # Positional encodings for both sides; keep max_len equal to MAX_SRC_LEN/TGT_LEN as appropriate\n",
    "        self.pos_src = PositionalEncoding(d_model, dropout, max_len=MAX_SRC_LEN)\n",
    "        self.pos_tgt = PositionalEncoding(d_model, dropout, max_len=MAX_TGT_LEN)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=enc_layers,\n",
    "            num_decoder_layers=dec_layers,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.output = nn.Linear(d_model, vocab_size, bias=False)\n",
    "        # weight tying: share decoder embedding and output projection\n",
    "        self.output.weight = self.tgt_embed.weight\n",
    "\n",
    "    @staticmethod\n",
    "    def _look_ahead_mask(size, device):\n",
    "        \"\"\"Return a boolean mask of shape [size, size] where True blocks attention to future positions.\"\"\"\n",
    "        return torch.triu(torch.ones(size, size, dtype=torch.bool, device=device), diagonal=1)\n",
    "\n",
    "    def forward(self, src, tgt_in):\n",
    "        # src: [B, S]; tgt_in: [B, T]\n",
    "        src_pad_mask = (src == PAD_ID)      # [B, S] bool\n",
    "        tgt_pad_mask = (tgt_in == PAD_ID)   # [B, T] bool\n",
    "        tgt_mask = self._look_ahead_mask(tgt_in.size(1), src.device)  # [T, T] bool\n",
    "\n",
    "        src_emb = self.pos_src(self.src_embed(src))\n",
    "        tgt_emb = self.pos_tgt(self.tgt_embed(tgt_in))\n",
    "\n",
    "        out = self.transformer(\n",
    "            src=src_emb,\n",
    "            tgt=tgt_emb,\n",
    "            tgt_mask=tgt_mask,\n",
    "            src_key_padding_mask=src_pad_mask,\n",
    "            tgt_key_padding_mask=tgt_pad_mask,\n",
    "            memory_key_padding_mask=src_pad_mask\n",
    "        )\n",
    "        logits = self.output(out)  # [B, T, V]\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431a01a3-4110-45e0-a97f-3e0ecadb20a5",
   "metadata": {},
   "source": [
    "##### We use boolean tgt_mask (look-ahead) to avoid PyTorch deprecation warnings caused by mismatched mask types. Weight tying reduces parameters and often improves learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9ef36d-e1a6-4a96-a391-a7d80a9b3344",
   "metadata": {},
   "source": [
    "## 9) Instantiate model and training utilities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ef5175b1-30f2-4d0d-87ac-65a0426f3fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 19559936\n"
     ]
    }
   ],
   "source": [
    "\n",
    "VOCAB_SIZE_ACTUAL = sp.get_piece_size()  # use actual trained vocabulary\n",
    "model = TransformerNMT(VOCAB_SIZE_ACTUAL, D_MODEL, NHEAD, ENC_LAY, DEC_LAY, FF_DIM, DROPOUT).to(device)\n",
    "\n",
    "# Loss with label smoothing (improves generalization)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_ID, label_smoothing=LABEL_SMOOTHING)\n",
    "\n",
    "# Optimizer: AdamW is stable for Transformers and works well with weight decay\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR_BASE, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Noam learning rate schedule (the original Transformer schedule with warmup)\n",
    "class NoamLR(optim.lr_scheduler._LRScheduler):\n",
    "    def __init__(self, optimizer, d_model, warmup_steps, last_epoch=-1):\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "        super().__init__(optimizer, last_epoch)\n",
    "    def get_lr(self):\n",
    "        step = max(self._step_count, 1)\n",
    "        scale = (self.d_model ** -0.5) * min(step ** -0.5, step * (self.warmup_steps ** -1.5))\n",
    "        return [LR_BASE * scale for _ in self.optimizer.param_groups]\n",
    "\n",
    "scheduler = NoamLR(optimizer, D_MODEL, WARMUP_STEPS)\n",
    "\n",
    "# AMP scaler (GPU only). If CPU, set to None.\n",
    "scaler = torch.cuda.amp.GradScaler() if device == \"cuda\" else None\n",
    "\n",
    "print(\"Model parameters:\", sum(p.numel() for p in model.parameters()) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa8f0b1-c752-4d5e-88bb-1516bbe30515",
   "metadata": {},
   "source": [
    "##### Noam schedule is well-suited for Transformers. On small datasets, simpler schedulers (ReduceLROnPlateau) also work; Noam helps with initial warmup then decay behavior. AMP reduces memory & speeds up training on CUDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675b2a28-18b9-4d78-8c31-36a5ddbb2dd8",
   "metadata": {},
   "source": [
    "## 10) Helpers: masked accuracy and decode helpers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42951ecb-a566-49d8-8ee5-76f7cf49637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def masked_token_accuracy(logits, targets):\n",
    "    \"\"\"Compute token-level accuracy ignoring padding.\"\"\"\n",
    "    preds = logits.argmax(dim=-1)  # [B, T]\n",
    "    mask = (targets != PAD_ID)\n",
    "    correct = (preds[mask] == targets[mask]).sum().item()\n",
    "    total = mask.sum().item()\n",
    "    return correct / max(total, 1)\n",
    "\n",
    "def decode_ids(ids):\n",
    "    \"\"\"Convert a list of token ids to a string, trimming BOS/EOS and PAD.\"\"\"\n",
    "    # remove BOS and trailing EOS/PAD\n",
    "    if isinstance(ids, torch.Tensor):\n",
    "        ids = ids.tolist()\n",
    "    # remove leading BOS if present\n",
    "    if len(ids) and ids[0] == BOS_ID:\n",
    "        ids = ids[1:]\n",
    "    # cut at EOS if present\n",
    "    if EOS_ID in ids:\n",
    "        ids = ids[:ids.index(EOS_ID)]\n",
    "    # drop PADs\n",
    "    ids = [i for i in ids if i not in (PAD_ID, BOS_ID, EOS_ID)]\n",
    "    return sp.decode(ids)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2a9f7a-5eed-49b3-b05f-a6a793c4a2e7",
   "metadata": {},
   "source": [
    "##### Tokenlevel accuracy is an interpretable proxy; BLEU is recommended for sentence level quality assessment (we include optional BLEU evaluation below)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb32318-9385-4f8c-ac4c-4bfe3a88e1af",
   "metadata": {},
   "source": [
    "## 11) Training loop (gradient accumulation, AMP safe, live metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a4593b-b52c-4cc7-84bd-29bf5c7544c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAD_ACCUM_STEPS = GRAD_ACCUM_STEPS  # from hyperparameters above\n",
    "best_val_acc = 0.0\n",
    "wait = 0\n",
    "\n",
    "def run_epoch(dataloader, training=True):\n",
    "    model.train(training)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    steps = 0\n",
    "\n",
    "    if training:\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        accum = 0\n",
    "\n",
    "    pbar = tqdm(dataloader)\n",
    "    for src, tgt_in, tgt_out in pbar:\n",
    "        src = src.to(device); tgt_in = tgt_in.to(device); tgt_out = tgt_out.to(device)\n",
    "\n",
    "        # Autocast (GPU only)\n",
    "        if scaler is not None:\n",
    "            autocast_ctx = torch.cuda.amp.autocast()\n",
    "        else:\n",
    "            # Dummy context manager for CPU to keep code uniform\n",
    "            class _DummyCtx:\n",
    "                def __enter__(self): return None\n",
    "                def __exit__(self, exc_type, exc, tb): return False\n",
    "            autocast_ctx = _DummyCtx()\n",
    "\n",
    "        with autocast_ctx:\n",
    "            logits = model(src, tgt_in)                     # [B, T, V]\n",
    "            loss = criterion(logits.transpose(1, 2), tgt_out)  # CE expects [B, V, T]\n",
    "\n",
    "        if training:\n",
    "            # scale loss for accumulation\n",
    "            if scaler is not None:\n",
    "                scaler.scale(loss / GRAD_ACCUM_STEPS).backward()\n",
    "            else:\n",
    "                (loss / GRAD_ACCUM_STEPS).backward()\n",
    "            accum += 1\n",
    "\n",
    "            if accum == GRAD_ACCUM_STEPS:\n",
    "                # gradient clipping and optimizer step\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "                if scaler is not None:\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "                accum = 0\n",
    "\n",
    "        acc = masked_token_accuracy(logits, tgt_out)\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc\n",
    "        steps += 1\n",
    "\n",
    "        # live display\n",
    "        pbar.set_postfix(loss=loss.item(), acc=acc)\n",
    "\n",
    "    # if training and leftover accum gradients exist, flush them\n",
    "    if training and 'accum' in locals() and accum > 0:\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), MAX_GRAD_NORM)\n",
    "        if scaler is not None:\n",
    "            scaler.step(optimizer); scaler.update()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    return total_loss / steps, total_acc / steps\n",
    "\n",
    "# Main training loop with early stopping\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    print(f\"\\nEpoch {epoch}/{EPOCHS}\")\n",
    "    train_loss, train_acc = run_epoch(train_loader, training=True)\n",
    "    val_loss, val_acc = run_epoch(valid_loader, training=False)\n",
    "\n",
    "    print(f\"Train Loss {train_loss:.3f} | Train Acc {train_acc:.4f}\")\n",
    "    print(f\"Val   Loss {val_loss:.3f} | Val   Acc {val_acc:.4f}\")\n",
    "\n",
    "    # Save best model\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        wait = 0\n",
    "        # Save state_dict (recommended over pickling full model)\n",
    "        torch.save(model.state_dict(), CKPT_PATH)\n",
    "        print(f\"New best model saved (Val Acc = {val_acc:.4f})\")\n",
    "    else:\n",
    "        wait += 1\n",
    "        if wait >= PATIENCE:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3893a473-30eb-43f6-89d9-25ee59ba6705",
   "metadata": {},
   "source": [
    "##### This training loop uses gradient accumulation for effective large batches while preventing OOM. We save the state_dict as the canonical checkpoint; that is portable and recommended over full model pickles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440ec5c6-75f5-436f-9790-1440f9279166",
   "metadata": {},
   "source": [
    "## 12) Load best model and evaluate on test set (token accuracy and BLEU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ea514e-0609-4928-88dc-0671585e6bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load checkpoint\n",
    "state = torch.load(CKPT_PATH, map_location=device, weights_only=True)\n",
    "model.load_state_dict(state)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Token-level accuracy on test set\n",
    "total_loss = 0.0\n",
    "total_acc = 0.0\n",
    "steps = 0\n",
    "all_hypotheses = []\n",
    "all_references = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for src, tgt_in, tgt_out in tqdm(test_loader):\n",
    "        src = src.to(device); tgt_in = tgt_in.to(device); tgt_out = tgt_out.to(device)\n",
    "        logits = model(src, tgt_in)\n",
    "        loss = criterion(logits.transpose(1, 2), tgt_out)\n",
    "        acc = masked_token_accuracy(logits, tgt_out)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_acc += acc\n",
    "        steps += 1\n",
    "\n",
    "        # Collect for BLEU: greedy decode\n",
    "        preds = logits.argmax(dim=-1).cpu().numpy()  # [B, T]\n",
    "        for p, t in zip(preds, tgt_out.cpu().numpy()):\n",
    "            all_hypotheses.append(sp.decode([i for i in p.tolist() if i not in (PAD_ID, BOS_ID, EOS_ID)]))\n",
    "            all_references.append(sp.decode([i for i in t.tolist() if i not in (PAD_ID, BOS_ID, EOS_ID)]))\n",
    "\n",
    "token_acc = total_acc / steps\n",
    "avg_loss = total_loss / steps\n",
    "print(f\"\\nTest token accuracy: {token_acc:.4f} | Avg loss: {avg_loss:.3f}\")\n",
    "\n",
    "# BLEU (sacrebleu expects references as list of list)\n",
    "bleu = sacrebleu.corpus_bleu(all_hypotheses, [all_references])\n",
    "print(\"BLEU score (sacrebleu):\", round(bleu.score, 2))\n",
    "\n",
    "# - BLEU is a sentence-level metric that correlates imperfectly with perceived quality.\n",
    "# - Use human inspection and examples for final judgement.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e35ef6-b2f1-4f4a-91fe-e4b10e6bc66d",
   "metadata": {},
   "source": [
    "##### We compute both token-level accuracy and BLEU to provide both token-wise and sentence-level quality measures. BLEU here is computed on greedy outputs; beam search will improve BLEU and perceived quality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d8a31d-57b9-418c-8273-d7f0079a1f1e",
   "metadata": {},
   "source": [
    "## 13) Save model state_dict as pickle (portable backup) and ensure tokenizer files are present\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca71c39-76cd-453e-a572-4d5f3c6f61ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13) Save model state_dict as pickle (portable backup) and ensure tokenizer files are present\n",
    "\n",
    "with open(PICKLE_PATH, \"wb\") as f:\n",
    "    pickle.dump(model.state_dict(), f)\n",
    "\n",
    "# The SentencePiece model and vocab are already created by the training step; ensure they exist\n",
    "assert Path(SPM_MODEL).exists(), f\"{SPM_MODEL} not found.\"\n",
    "assert Path(SPM_VOCAB).exists(), f\"{SPM_VOCAB} not found.\"\n",
    "\n",
    "print(f\"Saved: {CKPT_PATH}, {PICKLE_PATH}, {SPM_MODEL}, {SPM_VOCAB}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (ML)",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
